{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'whisper' has no attribute 'load_model' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01melevenlabs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m play\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwhisper\u001b[39;00m\n",
      "File \u001b[1;32mh:\\ML\\GP\\FlexAi\\Ai\\whisper.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load the Whisper model (use \"base\", \"small\", \"medium\", or \"large\")\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase.en\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Audio settings\u001b[39;00m\n\u001b[0;32m     12\u001b[0m CHUNK \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m  \u001b[38;5;66;03m# Buffer size\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'whisper' has no attribute 'load_model' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage , AIMessage\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from RAG import return_rag_chain\n",
    "\n",
    "from elevenlabs.client import ElevenLabs\n",
    "from elevenlabs import play\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hossa\\miniconda3\\envs\\py310\\lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "c:\\Users\\Hossa\\miniconda3\\envs\\py310\\lib\\site-packages\\whisper\\model.py:124: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  a = scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed Text:  Hello, can you tell me what is the best source of protein in order to increase my muscles?\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"base.en\")  # Options: \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n",
    "\n",
    "def transcribe_long_audio(file_path):\n",
    "    result = model.transcribe(file_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "# Example usage\n",
    "audio_file = \"Record (online-voice-recorder.com).mp3\"\n",
    "transcribed_text = transcribe_long_audio(audio_file)\n",
    "print(\"Transcribed Text:\", transcribed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model_size = \"small\"\n",
    "model = WhisperModel(model_size, device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s -> 6.00s]  Hello, can you tell me what is the best source of protein in order to increase my muscles?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run on GPU with FP16\n",
    "\n",
    "# or run on GPU with INT8\n",
    "# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
    "# or run on CPU with INT8\n",
    "# model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "\n",
    "segments, info = model.transcribe(\"Record (online-voice-recorder.com).mp3\", language=\"en\",beam_size=5)\n",
    "\n",
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'whisper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mwhisper\u001b[49m\u001b[38;5;241m.\u001b[39mList)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'whisper' is not defined"
     ]
    }
   ],
   "source": [
    "print(whisper.List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elevenlabs import  stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml') as file:\n",
    "    config = yaml.load(file, Loader=SafeLoader)\n",
    "elevenlabs_api_key = config['elevnlabs']['apiKey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'generate' from 'elevenlabs' (c:\\Users\\Hossa\\miniconda3\\envs\\py310\\lib\\site-packages\\elevenlabs\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01melevenlabs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'generate' from 'elevenlabs' (c:\\Users\\Hossa\\miniconda3\\envs\\py310\\lib\\site-packages\\elevenlabs\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from elevenlabs import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ELEVENLABS_API_KEY\"] = elevenlabs_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ElevenLabs(api_key=elevenlabs_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created successfully\n"
     ]
    }
   ],
   "source": [
    "rag=return_rag_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RunnableSequence' object has no attribute 'pick_chain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m response\u001b[38;5;241m=\u001b[39mrag\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtell what is the best way to increase my gym session time as i get tired so fast.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: []})\n\u001b[1;32m----> 2\u001b[0m rag_chain\u001b[38;5;241m=\u001b[39m\u001b[43mrag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpick_chain\u001b[49m(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Hossa\\miniconda3\\envs\\py310\\lib\\site-packages\\langchain_core\\runnables\\base.py:5745\u001b[0m, in \u001b[0;36mRunnableBinding.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5744\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m-> 5745\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5747\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(attr) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m   5748\u001b[0m         config_param \u001b[38;5;241m:=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(attr)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5749\u001b[0m     ):\n\u001b[0;32m   5750\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m config_param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m inspect\u001b[38;5;241m.\u001b[39mParameter\u001b[38;5;241m.\u001b[39mKEYWORD_ONLY:\n",
      "File \u001b[1;32mc:\\Users\\Hossa\\miniconda3\\envs\\py310\\lib\\site-packages\\pydantic\\main.py:856\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RunnableSequence' object has no attribute 'pick_chain'"
     ]
    }
   ],
   "source": [
    "response=rag.invoke({\"input\": \"tell what is the best way to increase my gym session time as i get tired so fast.\", \"chat_history\": []})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_responses(responses):\n",
    "    for response in responses:\n",
    "        print(response, end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag_chain=rag.pick('answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=rag_chain.stream({\"input\": \"tell what is the best way to increase my gym session time as i get tired so fast.\", \"chat_history\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input(\"Ask a question: \")\n",
    "response = rag.invoke({\"input\": question, \"chat_history\": []})[\"answer\"]\n",
    "\n",
    "audio = client.text_to_speech.convert(\n",
    "    text=response,\n",
    "    voice_id=\"JBFqnCBsd6RMkjVDRZzb\",\n",
    "    model_id=\"eleven_flash_v2\",\n",
    "    output_format=\"mp3_44100_128\",\n",
    ")\n",
    "play(audio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
